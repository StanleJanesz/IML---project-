{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54913130-01b4-4efd-96d0-a2ad96fb4896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import torchvision.transforms.v2\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78e6ec4f-bba5-4929-83a4-c0bc01450fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "05823567-8b1e-49d4-9499-f66e90a268e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bc81b007-24dd-4c8c-ab68-4d25aca0e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader):\n",
    "    classes = (\"0\", \"1\")\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    correct_pred = {classname: 0 for classname in classes}\n",
    "    total_pred = {classname: 0 for classname in classes}\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[classes[label]] += 1\n",
    "                total_pred[classes[label]] += 1\n",
    "\n",
    "    print(f'Accuracy of the network on the 1000 test images: {100 * correct // total} %')\n",
    "    # print accuracy for each class\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %, correct: {correct_count}, total: {total_pred[classname]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "85066162-98e0-4905-b792-075ef5f2ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_f1(correct_one, total_one, correct_zero, total_zero):\n",
    "    # Calculate precision, recall, and F1 score for class 0\n",
    "    true_positive_0 = correct_zero\n",
    "    false_positive_0 = total_one - correct_one\n",
    "    false_negative_0 = total_zero - correct_zero\n",
    "\n",
    "    precision_0 = true_positive_0 / (true_positive_0 + false_positive_0)\n",
    "    recall_0 = true_positive_0 / (true_positive_0 + false_negative_0)\n",
    "    f1_score_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0)\n",
    "\n",
    "    # Calculate precision, recall, and F1 score for class 1\n",
    "    true_positive_1 = correct_one\n",
    "    false_positive_1 = false_negative_0 \n",
    "    false_negative_1 = false_positive_0\n",
    "\n",
    "    precision_1 = true_positive_1 / (true_positive_1 + false_positive_1)\n",
    "    recall_1 = true_positive_1 / (true_positive_1 + false_negative_1)\n",
    "    f1_score_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1)\n",
    "\n",
    "    # Calculate macro average F1 score\n",
    "    macro_avg_f1_score = (f1_score_0 + f1_score_1) / 2\n",
    "\n",
    "    return macro_avg_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "05286576-fbe1-4234-b799-519c75bca3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.v2.Compose([transforms.Resize((96, 194)),transforms.v2.ToImage(),\\\n",
    "                                               transforms.v2.ToDtype(torch.float32, scale=True),\\\n",
    "                                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "trainset = ImageDataset(annotations_file = \"..\\\\work_folder\\\\daps_Data\\\\Spectrograms_mel_cropped_training\\\\labels.csv\", img_dir = \"..\\\\work_folder\\\\daps_Data\\\\Spectrograms_mel_cropped_training\\\\Images\"\\\n",
    "                       , transform = transform)\n",
    "\n",
    "testset = ImageDataset(annotations_file = \"..\\\\work_folder\\\\daps_Data\\\\Spectrograms_mel_cropped_test\\\\labels.csv\", img_dir = \"..\\\\work_folder\\\\daps_Data\\\\Spectrograms_mel_cropped_test\\\\Images\"\\\n",
    "                      , transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8d5a92d2-a429-416c-b976-eecb8fde4d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10  # Adjust based on training speed\n",
    "LEARNING_RATE = 0.001  # Assuming square images, resize to 64x64\n",
    "NUM_CLASSES = 2  # Change this based on your dataset\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 64\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "testloader = DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "classes = (\"0\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b6830dc7-9928-4496-8437-9fb697038e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),  # (N, 32, 64, 64)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (N, 32, 32, 32)\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # (N, 64, 32, 32)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (N, 64, 16, 16)\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # (N, 128, 16, 16)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # (N, 128, 8, 8)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(36864, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9fa7545d-1bda-4f1b-aba8-7734768cb6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = '../work_folder/model_4.pth'\n",
    "net = Net(2)\n",
    "net.load_state_dict(torch.load(PATH, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44c9a48a-2457-488d-bd7e-06c4ca96f16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1000 test images: 91 %\n",
      "Accuracy for class: 0     is 97.9 %, correct: 9949, total: 10166\n",
      "Accuracy for class: 1     is 77.4 %, correct: 3519, total: 4549\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(net, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "eb3d48b5-e648-463e-a317-e8d07e1c789e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8952566360416545"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f1(3519, 4549, 9949, 10166)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "555d2141-36ac-47ca-ad6e-66676a09f2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 81177 training images: 92 %\n",
      "Accuracy for class: 0     is 98.8 %, correct: 55459, total: 56130\n",
      "Accuracy for class: 1     is 78.3 %, correct: 19604, total: 25047\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(net, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cb0cd915-4a1b-4d19-b878-d938abbb8eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9064282528926193"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f1(19604, 25047, 55459, 56130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "498968cd-dc17-4d0c-895a-ba51b8c7be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = ImageDataset(annotations_file = \"..\\\\work_folder\\\\daps_Data\\\\Spectrograms2_cropped_training\\\\labels.csv\", img_dir = \"..\\\\work_folder\\\\daps_Data\\\\Spectrograms2_cropped_training\\\\Images\"\\\n",
    "                       , transform = transform)\n",
    "\n",
    "testset = ImageDataset(annotations_file = \"..\\\\work_folder\\\\daps_Data\\\\Spectrograms2_cropped_test\\\\labels.csv\", img_dir = \"..\\\\work_folder\\\\daps_Data\\\\Spectrograms2_cropped_test\\\\Images\"\\\n",
    "                      , transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9e2d4c28-4a9b-454e-9a2b-542a19cbc0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "testloader = DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2fd27afe-4e5c-4ad1-b85f-a5a6e53dd4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = '../work_folder/model_5.pth'\n",
    "net = Net(2)\n",
    "net.load_state_dict(torch.load(PATH, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14114f2f-503b-42b8-b43e-dea7fb3b5c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1000 test images: 94 %\n",
      "Accuracy for class: 0     is 95.7 %, correct: 9662, total: 10096\n",
      "Accuracy for class: 1     is 93.1 %, correct: 4236, total: 4549\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(net, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a21f3b5c-fecc-4ce0-92fc-9e6cf17b8012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9408769061825313"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f1(4236, 4549, 9662, 10096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "41e02f13-c88b-46bd-a198-31261cb228af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 81177 training images: 96 %\n",
      "Accuracy for class: 0     is 97.4 %, correct: 54696, total: 56141\n",
      "Accuracy for class: 1     is 93.7 %, correct: 23455, total: 25036\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(net, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a98da191-ab04-4be6-bde9-f64db0acaaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9562425187735828"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f1(23455, 25036, 54696, 56141)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9d6390ce-533c-403e-9228-3fed0e97b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = ImageDataset(annotations_file = \"..\\\\work_folder\\\\daps_Data\\\\Spectrograms_cropped_training\\\\labels.csv\", img_dir = \"..\\\\work_folder\\\\daps_Data\\\\Spectrograms_cropped_training\\\\Images\"\\\n",
    "                       , transform = transform)\n",
    "\n",
    "testset = ImageDataset(annotations_file = \"..\\\\work_folder\\\\daps_Data\\\\Spectrograms_cropped_test\\\\labels.csv\", img_dir = \"..\\\\work_folder\\\\daps_Data\\\\Spectrograms_cropped_test\\\\Images\"\\\n",
    "                      , transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3da18541-4104-4973-b758-2123ea3051ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "testloader = DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5887fa64-23e9-47bd-be89-06957c5463f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = '../work_folder/model_6.pth'\n",
    "net = Net(2)\n",
    "net.load_state_dict(torch.load(PATH, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1824ae28-b0f0-4a6a-a19d-d9901951f085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1000 test images: 88 %\n",
      "Accuracy for class: 0     is 90.0 %, correct: 9146, total: 10166\n",
      "Accuracy for class: 1     is 84.9 %, correct: 3800, total: 4476\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(net, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fed24555-ca4c-44cc-b306-182956c0428e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.866352513745769"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f1(3800, 4476, 9146, 10166)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e905367a-5da8-48fb-9200-92a180b96e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 81328 training images: 88 %\n",
      "Accuracy for class: 0     is 91.4 %, correct: 51614, total: 56477\n",
      "Accuracy for class: 1     is 83.4 %, correct: 20729, total: 24851\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(net, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e2a06a03-71b3-40d5-a5b2-7b603060b478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8709036114921318"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f1(20729, 24851, 51614, 56477)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "829f42e9-8790-413a-b140-71c8475e9f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = ImageDataset(annotations_file = \"..\\\\work_folder\\\\daps_Data\\\\Spectrograms2_cropped_training\\\\labels.csv\", img_dir = \"..\\\\work_folder\\\\daps_Data\\\\Spectrograms2_cropped_training\\\\Images\"\\\n",
    "                       , transform = transform)\n",
    "\n",
    "testset = ImageDataset(annotations_file = \"..\\\\work_folder\\\\daps_Data\\\\Spectrograms2_cropped_test\\\\labels.csv\", img_dir = \"..\\\\work_folder\\\\daps_Data\\\\Spectrograms2_cropped_test\\\\Images\"\\\n",
    "                      , transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "10c52eda-c2fe-4080-8dfa-4b05183dd45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "testloader = DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "93fc046f-8bd9-40fc-b316-f3db444d1ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),  # (N, 32, 64, 64)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (N, 32, 32, 32)\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # (N, 64, 32, 32)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (N, 64, 16, 16)\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # (N, 128, 16, 16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # (N, 128, 8, 8)\n",
    "        )\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "658ec645-a03a-4449-9dba-3d40b8cbc85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = '../work_folder/model_7.pth'\n",
    "net = Net(2)\n",
    "net.load_state_dict(torch.load(PATH, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "56fca9c6-2ccc-4c49-845d-559dc0fed3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 14645 test images: 80 %\n",
      "Accuracy for class: 0     is 88.1 %, correct: 8892, total: 10096\n",
      "Accuracy for class: 1     is 64.4 %, correct: 2928, total: 4549\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(net, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "513a8645-9558-4395-af14-63a80fecc562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7687503134664098"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f1(2928, 4549, 8892, 10096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5158b202-c244-435b-9460-faec02bc1b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 81177 training images: 95 %\n",
      "Accuracy for class: 0     is 96.9 %, correct: 54393, total: 56141\n",
      "Accuracy for class: 1     is 92.2 %, correct: 23089, total: 25036\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(net, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d22a58c6-0a7e-4271-b16a-59aa427e779e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.946530911784752"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f1(23089, 25036, 54393, 56141)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b18c7a0d-11ac-40fd-9c98-fc865be30d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.v2.Compose([transforms.Resize((193, 388)),transforms.v2.ToImage(),\\\n",
    "                                               transforms.v2.ToDtype(torch.float32, scale=True),\\\n",
    "                                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = ImageDataset(annotations_file = \"..\\\\work_folder\\\\daps_Data\\\\Spectrograms2_cropped_training\\\\labels.csv\", img_dir = \"..\\\\work_folder\\\\daps_Data\\\\Spectrograms2_cropped_training\\\\Images\"\\\n",
    "                       , transform = transform)\n",
    "\n",
    "testset = ImageDataset(annotations_file = \"..\\\\work_folder\\\\daps_Data\\\\Spectrograms2_cropped_test\\\\labels.csv\", img_dir = \"..\\\\work_folder\\\\daps_Data\\\\Spectrograms2_cropped_test\\\\Images\"\\\n",
    "                      , transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0f7d58da-2f9f-4ac6-9cb3-b4b3cfab66ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "testloader = DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a319648a-034c-41c5-adc9-861edcd101b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),  # (N, 32, 64, 64)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (N, 32, 32, 32)\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # (N, 64, 32, 32)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (N, 64, 16, 16)\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # (N, 128, 16, 16)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # (N, 128, 8, 8)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(147456, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "98ba75dc-1b25-436a-8240-c84dfa548ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = '../work_folder/model_8.pth'\n",
    "net = Net(2)\n",
    "net.load_state_dict(torch.load(PATH, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7982f93d-a8aa-450e-a557-38b9c0c514bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 14645 test images: 94 %\n",
      "Accuracy for class: 0     is 95.8 %, correct: 9676, total: 10096\n",
      "Accuracy for class: 1     is 92.8 %, correct: 4223, total: 4549\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(net, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "04f89b1c-f627-47e2-b59f-ce075c3e7d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9408621752542221"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f1(4223, 4549, 9676, 10096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "04afd52c-cd55-4d7c-a6f1-c8bb067f45e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 81177 training images: 98 %\n",
      "Accuracy for class: 0     is 98.8 %, correct: 55469, total: 56141\n",
      "Accuracy for class: 1     is 97.1 %, correct: 24310, total: 25036\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(net, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "73bb0edd-f200-4e79-ad5e-a0c3d4ac5b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9798026267729204"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f1(24310, 25036, 55469, 56141)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5820e2e6-2c89-4519-a77a-ede2c422446b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
